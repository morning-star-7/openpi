{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# to get start\n",
                "- source .venv/bin/activate or use conda env\n",
                "- jupyter notebook --port 8899 --ip 0.0.0.0 [in new terminal, not in vscode]\n",
                "\n",
                "[recommand]\n",
                "- also, we can you conda env . then select env upper right.\n",
                "- In MPK, use ```conda activate openpi-droid```\n",
                "\n",
                "### install pyzed\n",
                "https://www.stereolabs.com/docs/development/python/install\n",
                "\n",
                "(1) install ZED SDK\n",
                "\n",
                "(2)\n",
                "```cd \"/usr/local/zed/\" python3 get_python_api.py```\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# pi0 deployment details\n",
                "- wrist camera & its mount\n",
                "- normalize gripper state\n",
                "- image resilution: In droid: (180, 320, 3). "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### select location"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LOCATION = 'MPK'\n",
                "LOCATION = 'FRE'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import dataclasses\n",
                "\n",
                "import jax\n",
                "\n",
                "from openpi.models import model as _model\n",
                "from openpi.policies import droid_policy\n",
                "from openpi.policies import policy_config as _policy_config\n",
                "from openpi.shared import download\n",
                "from openpi.training import config as _config\n",
                "from openpi.training import data_loader as _data_loader\n",
                "\n",
                "import Pyro5.api\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# add roboarena checkpoints"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```\n",
                "### Trained from PaliGemma, using RT-2 / OpenVLA style binning tokenizer.\n",
                "uv run scripts/serve_policy.py policy:checkpoint --policy.config=paligemma_binning_droid --policy.dir=gs://openpi-assets/checkpoints/roboarena/paligemma_binning_droid\n",
                "\n",
                "### Trained from PaliGemma, using FAST tokenizer (using universal FAST+ tokenizer).\n",
                "uv run scripts/serve_policy.py policy:checkpoint --policy.config=paligemma_fast_droid --policy.dir=gs://openpi-assets/checkpoints/roboarena/paligemma_fast_droid\n",
                "\n",
                "### Trained from PaliGemma, using FAST tokenizer (tokenizer trained on DROID dataset).\n",
                "uv run scripts/serve_policy.py policy:checkpoint --policy.config=paligemma_fast_specialist_droid --policy.dir=gs://openpi-assets/checkpoints/roboarena/paligemma_fast_specialist_droid\n",
                "\n",
                "### Trained from PaliGemma, using FSQ tokenizer.\n",
                "uv run scripts/serve_policy.py policy:checkpoint --policy.config=paligemma_vq_droid --policy.dir=gs://openpi-assets/checkpoints/roboarena/paligemma_vq_droid\n",
                "\n",
                "### pi0-style diffusion / flow VLA, trained on DROID from PaliGemma.\n",
                "uv run scripts/serve_policy.py policy:checkpoint --policy.config=paligemma_diffusion_droid --policy.dir=gs://openpi-assets/checkpoints/roboarena/paligemma_diffusion_droid\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# load model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### pi0_droid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = _config.get_config(\"pi0_droid\")\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/pi0_droid\")\n",
                "\n",
                "# Create a trained policy.\n",
                "policy = _policy_config.create_trained_policy(config, checkpoint_dir)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### pi0_FAST_droid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = _config.get_config(\"pi0_fast_droid\")\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/pi0_fast_droid\")\n",
                "\n",
                "# Create a trained policy.\n",
                "policy = _policy_config.create_trained_policy(config, checkpoint_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### paligemma_binning_droid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = _config.get_config(\"paligemma_binning_droid\")\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/roboarena/paligemma_binning_droid\")\n",
                "\n",
                "# Create a trained policy.\n",
                "policy = _policy_config.create_trained_policy(config, checkpoint_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### paligemma_diffusion_droid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = _config.get_config(\"paligemma_diffusion_droid\")\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/roboarena/paligemma_diffusion_droid\")\n",
                "\n",
                "# Create a trained policy.\n",
                "policy = _policy_config.create_trained_policy(config, checkpoint_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### run it if you want to switch policy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# delete current policy to free up memory, then load a new one\n",
                "del policy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### sanity check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference on a dummy example. This example corresponds to observations produced by the DROID runtime.\n",
                "example = droid_policy.make_droid_example()\n",
                "result = policy.infer(example)\n",
                "print(\"Actions shape:\", result[\"actions\"].shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# initialize realsense external cameras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyrealsense2 as rs\n",
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "\n",
                "class RealSenseCamera:\n",
                "    def __init__(self, serial_number, width=320, height=180, fps=30):\n",
                "        self.serial = serial_number\n",
                "        self.pipeline = rs.pipeline()\n",
                "        self.config = rs.config()\n",
                "        self.config.enable_device(self.serial)\n",
                "        self.config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
                "        self.pipeline.start(self.config)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "    def get_image(self):\n",
                "        frames = self.pipeline.wait_for_frames()\n",
                "        color_frame = frames.get_color_frame()\n",
                "        if not color_frame:\n",
                "            return None\n",
                "\n",
                "        bgr = np.asanyarray(color_frame.get_data())\n",
                "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
                "        return rgb\n",
                "\n",
                "    # def get_image(self):\n",
                "    #     frames = self.pipeline.wait_for_frames()\n",
                "    #     color_frame = frames.get_color_frame()\n",
                "    #     if not color_frame:\n",
                "    #         return None\n",
                "        \n",
                "    #     bgr = np.asanyarray(color_frame.get_data())\n",
                "    #     rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "    #     # Center crop to (180, 320) - height=180, width=320\n",
                "    #     h, w = rgb.shape[:2]\n",
                "    #     target_h, target_w = 180, 320\n",
                "        \n",
                "    #     # Calculate crop coordinates\n",
                "    #     start_y = (h - target_h) // 2\n",
                "    #     start_x = (w - target_w) // 2\n",
                "    #     end_y = start_y + target_h\n",
                "    #     end_x = start_x + target_w\n",
                "        \n",
                "    #     # Perform center crop\n",
                "    #     rgb_cropped = rgb[start_y:end_y, start_x:end_x]\n",
                "        \n",
                "    #     return rgb_cropped\n",
                "\n",
                "\n",
                "    def release(self):\n",
                "        self.pipeline.stop()\n",
                "\n",
                "\n",
                "\n",
                "# For FRE\n",
                "# camera serial numbers\n",
                "if LOCATION == \"FRE\":\n",
                "    serial_left = \"934222071783\"\n",
                "    serial_right = \"925622070414\"\n",
                "elif LOCATION == \"MPK\":\n",
                "    serial_left = \"948522071060\"\n",
                "    serial_right = \"838212074411\"\n",
                "\n",
                "# # initialize cameras\n",
                "cam_left = RealSenseCamera(serial_left)\n",
                "cam_right = RealSenseCamera(serial_right)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# inicialize zed mini wrist camera"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyzed.sl as sl\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "class ZEDMiniCamera:\n",
                "    def __init__(self, resolution=sl.RESOLUTION.HD720, fps=30, depth=False):\n",
                "        print(\"[INFO] Initializing ZED Mini camera\")\n",
                "        self.zed = sl.Camera()\n",
                "\n",
                "        init_params = sl.InitParameters()\n",
                "        init_params.camera_resolution = resolution\n",
                "        init_params.camera_fps = fps\n",
                "        init_params.depth_mode = sl.DEPTH_MODE.PERFORMANCE if depth else sl.DEPTH_MODE.NONE\n",
                "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # For depth, if used\n",
                "\n",
                "        status = self.zed.open(init_params)\n",
                "        if status != sl.ERROR_CODE.SUCCESS:\n",
                "            raise RuntimeError(f\"[ERROR] Failed to open ZED camera: {status}\")\n",
                "        print(\"[SUCCESS] ZED camera opened successfully\")\n",
                "\n",
                "        self.image = sl.Mat()\n",
                "\n",
                "    def get_image(self):\n",
                "        if self.zed.grab() == sl.ERROR_CODE.SUCCESS:\n",
                "            self.zed.retrieve_image(self.image, sl.VIEW.LEFT)  # LEFT image (color)\n",
                "            bgr_image = self.image.get_data()\n",
                "            rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
                "            return rgb_image\n",
                "        else:\n",
                "            print(\"[WARN] Failed to grab frame from ZED\")\n",
                "            return None\n",
                "\n",
                "\n",
                "    # def get_image(self):\n",
                "    #     if self.zed.grab() == sl.ERROR_CODE.SUCCESS:\n",
                "    #         self.zed.retrieve_image(self.image, sl.VIEW.LEFT)  # LEFT image (color)\n",
                "    #         bgr_image = self.image.get_data()\n",
                "    #         rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
                "            \n",
                "    #         # Center crop to (180, 320) - height=180, width=320\n",
                "    #         h, w = rgb_image.shape[:2]\n",
                "    #         target_h, target_w = 180, 320\n",
                "            \n",
                "    #         # Calculate crop coordinates\n",
                "    #         start_y = (h - target_h) // 2\n",
                "    #         start_x = (w - target_w) // 2\n",
                "    #         end_y = start_y + target_h\n",
                "    #         end_x = start_x + target_w\n",
                "            \n",
                "    #         # Perform center crop\n",
                "    #         rgb_cropped = rgb_image[start_y:end_y, start_x:end_x]\n",
                "            \n",
                "    #         return rgb_cropped\n",
                "    #     else:\n",
                "    #         print(\"[WARN] Failed to grab frame from ZED\")\n",
                "    #         return None\n",
                "\n",
                "    def release(self):\n",
                "        self.zed.close()\n",
                "        print(\"[INFO] ZED camera released\")\n",
                "\n",
                "\n",
                "zed_cam = ZEDMiniCamera(resolution=sl.RESOLUTION.VGA, fps=30)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## get current images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from openpi_client import image_tools\n",
                "\n",
                "img_left = image_tools.resize_with_pad(\n",
                "                        cam_left.get_image(), 224, 224\n",
                "                    )\n",
                "# img_wrist = image_tools.resize_with_pad(\n",
                "#                         cam_wrist.get_image(), 224, 224\n",
                "#                     )\n",
                "\n",
                "img_wrist = image_tools.resize_with_pad(\n",
                "                        zed_cam.get_image(), 224, 224\n",
                "                    )\n",
                "\n",
                "img_right = image_tools.resize_with_pad(\n",
                "                        cam_right.get_image(), 224, 224\n",
                "                    )\n",
                "\n",
                "# combine the two images\n",
                "combined = np.hstack((img_left, img_wrist, img_right))  # shape (224, 672, 3)\n",
                "\n",
                "# display the combined image\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.imshow(combined)\n",
                "plt.title(\"Left + Wrist + Right (RGB 224x224)\")\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# release cameras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cam_left.release()\n",
                "cam_right.release()\n",
                "zed_cam.release()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# openpi inference client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# vla_policy_client\n",
                "import Pyro5.api\n",
                "import numpy as np\n",
                "import time\n",
                "from openpi_client import image_tools\n",
                "\n",
                "ns = Pyro5.api.locate_ns()  # Locate the name server\n",
                "uri = ns.lookup(\"pi0_controller\")  # Use the name server to look up the URI\n",
                "controller = Pyro5.api.Proxy(uri)\n",
                "\n",
                "# prompt\n",
                "prompt = \"pick up the red bowl\"  # <-- change prompt\n",
                "\n",
                "MAX_STEPS = 50000  # maximum number of steps to run\n",
                "video_buffer = []  # buffer to store video frames\n",
                "\n",
                "# dummy action\n",
                "action = np.zeros((10,8), dtype=np.float32)\n",
                "action_list = action.tolist()  # convert to list for sending\n",
                "\n",
                "for step in range(MAX_STEPS):\n",
                "    start = time.time()\n",
                "    print(f\"\\n=== Step {step} ===\")\n",
                "    data_to_send = {\n",
                "        \"action\": action_list,\n",
                "        \"step\": step\n",
                "    }\n",
                "    start1 = time.time()\n",
                "    print(f\"Sending data to controller: {data_to_send}\")\n",
                "    obs = controller.step(data_to_send)\n",
                "    end1 = time.time()\n",
                "    print(f\"Controller step took {end1 - start1:.2f} seconds\")\n",
                "\n",
                "    img_left = image_tools.resize_with_pad(\n",
                "                            cam_left.get_image(), 224, 224\n",
                "                        )\n",
                "    img_wrist = image_tools.resize_with_pad(\n",
                "                            zed_cam.get_image(), 224, 224\n",
                "                        )\n",
                "    img_right = image_tools.resize_with_pad(\n",
                "                            cam_right.get_image(), 224, 224\n",
                "                        )\n",
                "    \n",
                "    # save the images\n",
                "    combined = np.hstack([img_left, img_right, img_wrist])\n",
                "    video_buffer.append(combined)\n",
                "\n",
                "    observation = {\n",
                "        \"observation/exterior_image_1_left\": img_left,\n",
                "        # \"observation/exterior_image_1_left\": np.zeros_like(img_left),  # dummy image for left\n",
                "        \"observation/wrist_image_left\": img_wrist,\n",
                "        # \"observation/wrist_image_left\": np.zeros_like(img_wrist),  # dummy image for wrist\n",
                "        \"observation/joint_position\": np.array(obs[\"robot_state\"], dtype=np.float32),\n",
                "        \"observation/gripper_position\": np.array([obs[\"gripper_state\"]], dtype=np.float32),  \n",
                "        # \"observation/joint_position\": np.zeros_like(obs[\"robot_state\"], dtype=np.float32),\n",
                "        # \"observation/gripper_position\": np.zeros_like([obs[\"gripper_state\"]], dtype=np.float32),  \n",
                "        \"prompt\": prompt,\n",
                "    }\n",
                "\n",
                "    result = policy.infer(observation)\n",
                "    action_list = result[\"actions\"].tolist()\n",
                "    end2 = time.time()\n",
                "    print(f\"Inference took {end2 - end1:.2f} seconds\")\n",
                "    print(f\"Action at step {step}:\", action_list[0])\n",
                "    end = time.time()\n",
                "    print(f\"Step {step} took {end - start:.2f} seconds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## save to gif"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import imageio\n",
                "import cv2\n",
                "\n",
                "\n",
                "gif_path = \"pi0_droid_drawer_1.gif\"\n",
                "imageio.mimsave(gif_path, video_buffer, duration=0.1)\n",
                "print(f\"GIF saved:{gif_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image\n",
                "Image(filename=\"pi0_droid_drawer_1.gif\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "openpi",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
